{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6a2769-2d5d-49f2-8562-2d3ac792f400",
   "metadata": {},
   "source": [
    "### __Inside a Single RNN Layer (Vanilla RNN)__\n",
    "\n",
    "A single RNN layer maintains temporal memory by combining the current input and the previous hidden state using three shared weight matrices, producing both a new hidden state and an output at every time step.\n",
    "\n",
    "RNNs machinery is a bit more complex. Inside a single Recurrent Neural Network layer we have `3 weight matrices` as well as `2 input tensors` and `2 output tensors`.\n",
    "\n",
    "\n",
    "\n",
    "At each time step t, a basic RNN cell computes ht(hidden state output): \n",
    "\n",
    "$$\n",
    "h_t = \\phi\\left(W_{xh} x_t + W_{hh} h_{t-1} + b_h\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a745dff-df87-40d8-9ec9-c93660a5b9ff",
   "metadata": {},
   "source": [
    "![](https://i.sstatic.net/xFs0V.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb9bb3-de5a-4904-a651-b08f4228b62a",
   "metadata": {},
   "source": [
    "- ϕ is a non-linear activation function such as tanh or ReLU\n",
    "- xt is the input at time step t\n",
    "- ht−1 is the previous hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e4118-0312-43c3-b9ff-cf0180ec70b3",
   "metadata": {},
   "source": [
    "__Weight Matrices (3 Total)__\n",
    "\n",
    "\n",
    "1. Input → Hidden\n",
    "   \n",
    "$$\n",
    "W_{xh} \\in \\mathbb{R}^{d_{\\text{hidden}} \\times d_{\\text{input}}}\n",
    "$$\n",
    "\n",
    "2. Hidden → Hidden (Recurrent Weight)\n",
    "\n",
    "$$\n",
    "W_{hh} \\in \\mathbb{R}^{d_{\\text{hidden}} \\times d_{\\text{hidden}}}\n",
    "$$\n",
    "\n",
    "\n",
    "3. Hidden → Output\n",
    "\n",
    "$$\n",
    "W_{hy} \\in \\mathbb{R}^{d_{\\text{output}} \\times d_{\\text{hidden}}}\n",
    "$$\n",
    "\n",
    "These weight matrices are shared across all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776bf99-c15c-44ce-9284-2d7def554a4a",
   "metadata": {},
   "source": [
    "__Input Tensors (2)__\n",
    "\n",
    "At time step t:\n",
    "\n",
    "1. Current input\n",
    "2. Previous hidden state\n",
    "\n",
    "\n",
    "__Output Tensors (2)__\n",
    "\n",
    "1. Current hidden state\n",
    "2. Current output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea10a4-5750-48ac-8727-e5fc0222f158",
   "metadata": {},
   "source": [
    "![](https://contenthub-static.grammarly.com/blog/wp-content/uploads/2024/09/158474-6180-Blog-Visuals_-RNNs_V1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabde7aa-ffaa-4d77-858d-d9f3f39ed0d4",
   "metadata": {},
   "source": [
    "Recurrent Nets introduce a new concept called “hidden state”, which is simply another input based on previous layer outputs. But wait, if this is based on previous layer outputs, how do I get it for the first run? Simple, just start it with zeros.\n",
    "\n",
    "RNNs are fed in a different way than feedforward networks. Because we are working with sequences, the order that we input the data matters, this is why each time we feed the net, we have to input a single item in the sequence. for example if it’s a stock price, we input the stock price for each day. If it’s a text we enter a single letter/word each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1c786-9fcb-4297-896b-9b5f88faeae1",
   "metadata": {},
   "source": [
    "### __Character RNN__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e73aab-a450-4725-834b-c867cb931559",
   "metadata": {},
   "source": [
    "A Character-level RNN (Char-RNN) is a recurrent neural network that models text one character at a time instead of word by word.\n",
    "\n",
    "Suppose your text is:\n",
    "\n",
    "\"hello\"\n",
    "\n",
    "\n",
    "Training pairs look like:\n",
    "\n",
    "| Input (chars) | Target (next char) |\n",
    "| ------------- | ------------------ |\n",
    "| h             | e                  |\n",
    "| e             | l                  |\n",
    "| l             | l                  |\n",
    "| l             | o                  |\n",
    "\n",
    "At each time step t:\n",
    "\n",
    "\n",
    "$$\n",
    "h_t = \\phi\\left(W_{xh} x_t + W_{hh} h_{t-1} + b_h\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n",
    "\n",
    "\n",
    "- x_t → one-hot or embedding of current character\n",
    "- h_t → hidden state (memory)\n",
    "- y_t → logits over all possible characters\n",
    "\n",
    "\n",
    "Text generation (sampling)\n",
    "\n",
    "- Start with a seed character (e.g. \"h\")\n",
    "- Predict next char probabilities\n",
    "- Sample one char\n",
    "- Feed it back as input\n",
    "- Repeat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685b3ea-72c3-4cde-b19d-dce0c1f58824",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcf896fc-1b26-43b9-9924-abf5ae69e070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np\n",
    "import random\n",
    "import requests \n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f389a6e9-530e-48da-812f-ec9b72b35a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    'mps' if torch.mps.is_available()\n",
    "    else 'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9306cb3-ce68-4923-963e-ba86117d5c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', 'I', 'P', 'a', 'm', 'n', 'w'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set('I am Pawan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ab2c8ee6-06cd-42db-bf78-9b7d0978e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_data: str, seq_length:int = 25) -> None:\n",
    "\n",
    "        self.chars = sorted(list(set(text_data)))\n",
    "        self.data_size, self.vocab_size = len(text_data), len(self.chars)\n",
    "            \n",
    "        self.inx_to_char = {i:ch for i, ch in enumerate(self.chars)}\n",
    "        self.char_to_idx = {ch:i for i, ch in enumerate(self.chars)}\n",
    "        self.seq_length = seq_length\n",
    "        self.X = self.string_to_vector(text_data)\n",
    "\n",
    "    @property\n",
    "    def X_string(self) -> str:\n",
    "        return self.vector_to_string(self.X)\n",
    "\n",
    "    def __len__(self) -> int: \n",
    "        return int(len(self.X)/self.seq_length -1)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.tensor, torch.tensor]:\n",
    "        start_idx = index * self.seq_length\n",
    "        end_idx = (index + 1) * self.seq_length\n",
    "\n",
    "        X = torch.tensor(self.X[start_idx:end_idx], dtype=torch.long)\n",
    "        y = torch.tensor(self.X[start_idx+1:end_idx+1], dtype=torch.long)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def string_to_vector(self, name:str) -> list[int]:\n",
    "        vector = list()\n",
    "        for s in name:\n",
    "            vector.append(self.char_to_idx[s])\n",
    "        return vector\n",
    "\n",
    "    def vector_to_string(self, vector: list[int]) -> str:\n",
    "        vector_string =  ''\n",
    "        for i in vector:\n",
    "            vector_string += self.inx_to_char[i]\n",
    "        return vector_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3465c6ab-15f0-47ab-87b0-cdbdd6dc97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Class \n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 batch_size: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size  = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        # self.i2h = nn.Linear(input_size, hidden_size, bias=False) \n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, X, hidden_state) -> tuple[torch.tensor, torch.tensor]:\n",
    "\n",
    "        X = self.embedding(X)\n",
    "        hidden_state = self.h2h(hidden_state)\n",
    "        hidden_state = torch.tanh(X + hidden_state)\n",
    "        return self.h2o(hidden_state), hidden_state\n",
    "\n",
    "    def init_zero_hidden(self, batch_size=1) -> torch.tensor:\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8dfd9d5f-8477-4a5d-97fa-f35df405adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(out, temperature=0.7):\n",
    "    out = out / temperature\n",
    "    probs = torch.softmax(out, dim=1)\n",
    "    return torch.multinomial(probs, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "14ad42df-4be3-4175-9e24-67659321aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: RNN, dataset: TextDataset, prediction_lengeth:int = 100) -> str:\n",
    "\n",
    "    model.eval()\n",
    "    predicted = dataset.vector_to_string([random.randint(0, len(dataset.chars) -1)])\n",
    "    hidden = model.init_zero_hidden()\n",
    "\n",
    "    for i in range(prediction_lengeth -1):\n",
    "        last_char = torch.tensor([dataset.char_to_idx[predicted[-1]]])\n",
    "        X, hidden = last_char.to(device), hidden.to(device)\n",
    "        out, hidden = model(X, hidden)\n",
    "        # result = torch.multinomial(nn.functional.softmax(out,1),1).item()\n",
    "        result = sample_with_temperature(out, temperature=0.7)\n",
    "        predicted += dataset.inx_to_char[result]\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "23e50a76-6dc8-4583-93fc-7d6ffc513017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:RNN, \n",
    "          data: DataLoader,\n",
    "          epochs: int,\n",
    "          optimizer: optim.Optimizer,\n",
    "          loss_fn: nn.Module) -> None:\n",
    "\n",
    "    train_losses = []\n",
    "    model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = list()\n",
    "\n",
    "        for X, y in data:\n",
    "            if X.shape[0] != model.batch_size:\n",
    "                continue\n",
    "            hidden = model.init_zero_hidden(batch_size=model.batch_size)\n",
    "\n",
    "            X, y, hidden  = X.to(device), y.to(device), hidden.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for c in range(X.shape[1]):\n",
    "                # out, hidden  = model(X[:, c].reshape(X.shape[0],1), hidden)\n",
    "                out, hidden = model(X[:, c], hidden)\n",
    "                loss += loss_fn(out, y[:,c].long())\n",
    "                \n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "\n",
    "\n",
    "        train_losses.append(torch.tensor(epoch_losses).mean())\n",
    "\n",
    "        print(f'epoch : {epoch + 1}, loss {train_losses[epoch]}')\n",
    "        print(generate_text(model, data.dataset))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cda72060-7d7b-4373-b98e-ec133c233e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path  = Path('datasets/')\n",
    "\n",
    "with open(data_path / 'dinos.txt', 'wb') as f:\n",
    "    request = requests.get('https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/refs/heads/master/dinos.txt')\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0ee0af1a-9e96-4319-a587-e8aa385162e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss 2.292391300201416\n",
      "hiobot\n",
      "ronosaurus\n",
      "lennteosaurus\n",
      "tinnitorosaurus\n",
      "sankonocaurus\n",
      "thinatoruvs\n",
      "uelopronges\n",
      "kilisaurus\n",
      "mni\n",
      "epoch : 2, loss 1.9377340078353882\n",
      "ysaurus\n",
      "churinn\n",
      "hhanimus\n",
      "saurososaurus\n",
      "teratops\n",
      "torathiwelt\n",
      "rus\n",
      "ptyraptor\n",
      "itarosaurus\n",
      "telitor\n",
      "niando\n",
      "epoch : 3, loss 1.8498417139053345\n",
      "inasaurus\n",
      "saondia\n",
      "tiodan\n",
      "sixa\n",
      "locavthn\n",
      "terhalodos\n",
      "singinasaurus\n",
      "tritops\n",
      "tagoyan\n",
      "saurus\n",
      "palichus\n",
      "qini\n",
      "epoch : 4, loss 1.8019977807998657\n",
      "x\n",
      "kelacosaurus\n",
      "kasaurs\n",
      "mauropyptomalonogacomanngor\n",
      "framtorataptrosaurus\n",
      "sugisaurus\n",
      "palianosaurus\n",
      "she\n",
      "epoch : 5, loss 1.7568279504776\n",
      "x\n",
      "tricomantor\n",
      "qchuansaurus\n",
      "meleosaurus\n",
      "roekanosaurus\n",
      "gdenanosaurus\n",
      "terachainos\n",
      "uracosaurus\n",
      "telanosau\n",
      "epoch : 6, loss 1.730265736579895\n",
      "onyx\n",
      "ptoria\n",
      "ranthosaurus\n",
      "tatipandyts\n",
      "protor\n",
      "kutheinasaurus\n",
      "sigtops\n",
      "tenatop\n",
      "liasaurus\n",
      "lyonosaurus\n",
      "tom\n",
      "epoch : 7, loss 1.7053937911987305\n",
      "x\n",
      "beneronamores\n",
      "aurasaurus\n",
      "cojisaurosaurus\n",
      "sineteosaurus\n",
      "richangianosaurus\n",
      "tiranenalia\n",
      "cplopongosaur\n",
      "epoch : 8, loss 1.6854609251022339\n",
      "yx\n",
      "auhualosaurus\n",
      "toraitapsaurus\n",
      "cincelecator\n",
      "pantansaurus\n",
      "qikaceratops\n",
      "gichosaurus\n",
      "shanosaurus\n",
      "tavin\n",
      "epoch : 9, loss 1.6639639139175415\n",
      "us\n",
      "kelesaurus\n",
      "teratops\n",
      "titenosaurus\n",
      "saliong\n",
      "konanosius\n",
      "stridon\n",
      "spteroitia\n",
      "vicansaurus\n",
      "trianteryx\n",
      "stt\n",
      "epoch : 10, loss 1.6474381685256958\n",
      "\n",
      "venathanosaurus\n",
      "huanlensaurus\n",
      "stitasaurus\n",
      "samistanosaurus\n",
      "puarichaeus\n",
      "titykus\n",
      "saurosaurus\n",
      "shiniosau\n",
      "epoch : 11, loss 1.631791114807129\n",
      "s\n",
      "bonosaurus\n",
      "palmiansaurus\n",
      "nonakanosaurus\n",
      "oropterosaurus\n",
      "ianosaurus\n",
      "teqiniansaurus\n",
      "minusachiania\n",
      "ori\n",
      "epoch : 12, loss 1.6153708696365356\n",
      "x\n",
      "sterasaurus\n",
      "schyarianosaurus\n",
      "pialesosaurus\n",
      "ticeratos\n",
      "linosaurus\n",
      "konatorasius\n",
      "masaurus\n",
      "siomosaurus\n",
      "\n",
      "epoch : 13, loss 1.6011682748794556\n",
      "rus\n",
      "jraphosaurus\n",
      "pniusaurus\n",
      "tithosaurus\n",
      "syptotazenosaurus\n",
      "lomolodon\n",
      "xisaurus\n",
      "tangaqanosaurus\n",
      "tholino\n",
      "epoch : 14, loss 1.5858968496322632\n",
      "venatops\n",
      "asaurus\n",
      "ninomosaurus\n",
      "pynaresaurus\n",
      "siniangkianfus\n",
      "greinosaurus\n",
      "steloraptor\n",
      "muspondenas\n",
      "strap\n",
      "epoch : 15, loss 1.5743190050125122\n",
      "yphosaurus\n",
      "magasaurus\n",
      "protoraptor\n",
      "miniangosaurus\n",
      "tadonlasaurus\n",
      "chinotarosaurus\n",
      "tachenosaurus\n",
      "taeswan\n",
      "epoch : 16, loss 1.5594463348388672\n",
      "zhiangosaurus\n",
      "torurisaurus\n",
      "prosaurus\n",
      "ritaton\n",
      "kurisaurus\n",
      "tatania\n",
      "shanchasaurus\n",
      "marialosaurus\n",
      "shanosau\n",
      "epoch : 17, loss 1.5486482381820679\n",
      "us\n",
      "mentale\n",
      "halbeobaraptor\n",
      "stecoceratops\n",
      "xiebjia\n",
      "surasaurus\n",
      "cannisaurus\n",
      "teraenosaurus\n",
      "taraetosaurus\n",
      "t\n",
      "epoch : 18, loss 1.5340242385864258\n",
      "jurus\n",
      "sinicona\n",
      "luchosaurus\n",
      "xiomasaurus\n",
      "mamparaptor\n",
      "auraptor\n",
      "basaurosaurus\n",
      "gianosaurus\n",
      "niahylnosaurus\n",
      "epoch : 19, loss 1.5235309600830078\n",
      "winus\n",
      "shualhylosaurus\n",
      "tazroptor\n",
      "meotoriosaurus\n",
      "shanosaurus\n",
      "tagys\n",
      "salosaurus\n",
      "provenatops\n",
      "gilenisaurus\n",
      "epoch : 20, loss 1.5099700689315796\n",
      "jurus\n",
      "megcolophus\n",
      "surocorasaurus\n",
      "hycheron\n",
      "opteryx\n",
      "catacosaurus\n",
      "kutia\n",
      "selmilus\n",
      "palusaurus\n",
      "scendeosaur\n",
      "epoch : 21, loss 1.5001401901245117\n",
      "lusaurus\n",
      "dylungosaurus\n",
      "prykonia\n",
      "s\n",
      "eucesaurus\n",
      "taenornitholdon\n",
      "eorocometsaurus\n",
      "nitavisaurus\n",
      "shuangnath\n",
      "epoch : 22, loss 1.4869680404663086\n",
      "acrosteon\n",
      "matawasaurus\n",
      "titesinosaurus\n",
      "shuroteryx\n",
      "saurus\n",
      "chianganaosaurus\n",
      "sitongosaurus\n",
      "bicerosaurus\n",
      "\n",
      "epoch : 23, loss 1.4745689630508423\n",
      "hulonondylux\n",
      "venosaurus\n",
      "shangoceratops\n",
      "protonia\n",
      "saurus\n",
      "shandasaurus\n",
      "lienyosaurus\n",
      "aptaronkus\n",
      "siurestr\n",
      "epoch : 24, loss 1.4645894765853882\n",
      "wisaurus\n",
      "vusanosaurus\n",
      "teryzong\n",
      "nibroneus\n",
      "sinohondoce\n",
      "hongylaeo\n",
      "lachinosaurus\n",
      "ohosaurus\n",
      "micesaurus\n",
      "ia\n",
      "epoch : 25, loss 1.451028823852539\n",
      "ia\n",
      "waleodon\n",
      "titanosaurus\n",
      "qiangkusaurus\n",
      "raetetosaurus\n",
      "molemisaurus\n",
      "schinotor\n",
      "brendongosaurus\n",
      "pinorera\n",
      "epoch : 26, loss 1.4416141510009766\n",
      "\n",
      "sarcothesaurus\n",
      "pyadondon\n",
      "sauromaysiemes\n",
      "palaphalesaurus\n",
      "shuwamiraptor\n",
      "penosaurus\n",
      "streptorondosaurus\n",
      "epoch : 27, loss 1.428083896636963\n",
      "dosaurus\n",
      "shunosaurus\n",
      "maeton\n",
      "tialoenptor\n",
      "fruinopldros\n",
      "atingosaurus\n",
      "siltesaurus\n",
      "matrhaneus\n",
      "tyracrostro\n",
      "epoch : 28, loss 1.4186815023422241\n",
      "jiansaurus\n",
      "sinosaurus\n",
      "chuasitan\n",
      "psaurus\n",
      "sthalesthan\n",
      "hynarsaurus\n",
      "qianzhokisaurus\n",
      "nachychelosaurus\n",
      "shu\n",
      "epoch : 29, loss 1.4054392576217651\n",
      "us\n",
      "protanrus\n",
      "dracooeratops\n",
      "prosaurosaurus\n",
      "prophisaurus\n",
      "dorimus\n",
      "surutitosaurus\n",
      "jiangosaurus\n",
      "modontero\n",
      "epoch : 30, loss 1.3928524255752563\n",
      "jiangaurus\n",
      "huisaurus\n",
      "iutosaurus\n",
      "scaleholosaurus\n",
      "eurornotarhus\n",
      "katerosaurus\n",
      "thodypsosaurus\n",
      "tanosaurus\n",
      "epoch : 31, loss 1.3841791152954102\n",
      "ponyanosaurus\n",
      "trillosaurus\n",
      "shanochosaurus\n",
      "chacrosaurus\n",
      "alaendia\n",
      "asaurus\n",
      "nolophosaurus\n",
      "thapsornithoid\n",
      "epoch : 32, loss 1.3702645301818848\n",
      "beitalong\n",
      "huachepsaurus\n",
      "sinophosaurus\n",
      "crischeoros\n",
      "trodenosaurus\n",
      "tacantholia\n",
      "torosaurus\n",
      "langdonosauru\n",
      "epoch : 33, loss 1.358096718788147\n",
      "ndosaurus\n",
      "siliosaurus\n",
      "tiurinosaurus\n",
      "anelosaurus\n",
      "silisaurus\n",
      "tornithoneus\n",
      "criatops\n",
      "trictrosaurus\n",
      "lenoc\n",
      "epoch : 34, loss 1.3490954637527466\n",
      "s\n",
      "vosaurus\n",
      "liatoploce\n",
      "saurosaurus\n",
      "meganosaurus\n",
      "siliasaurus\n",
      "luklongosaurus\n",
      "suemosaurus\n",
      "spinosaurus\n",
      "su\n",
      "epoch : 35, loss 1.334945797920227\n",
      "rosaurus\n",
      "shansaurus\n",
      "padrodontosaurus\n",
      "thenonychosaurus\n",
      "siucropops\n",
      "gganasaurus\n",
      "thaleita\n",
      "suchisaurus\n",
      "te\n",
      "epoch : 36, loss 1.3229424953460693\n",
      "\n",
      "rheitrosaurus\n",
      "silosaurus\n",
      "erentosaurus\n",
      "hudonysus\n",
      "s\n",
      "nethosaurus\n",
      "velaanosaurus\n",
      "sucholosaurus\n",
      "chilong\n",
      "s\n",
      "epoch : 37, loss 1.3137848377227783\n",
      "eratops\n",
      "crodontosaurus\n",
      "taranosaurus\n",
      "proberosaurus\n",
      "wulasaurus\n",
      "xiceratops\n",
      "arcodosaurus\n",
      "therosaurus\n",
      "pro\n",
      "epoch : 38, loss 1.2998076677322388\n",
      "quansaurus\n",
      "sulephosaurus\n",
      "lochengosaurus\n",
      "noshanopa\n",
      "sarkanycaudin\n",
      "saurusaurus\n",
      "selingosaurus\n",
      "thodonosau\n",
      "epoch : 39, loss 1.2878822088241577\n",
      "viceratops\n",
      "brchidon\n",
      "ornithoides\n",
      "suriodon\n",
      "osaurus\n",
      "shuscrastasaurus\n",
      "crothosaurus\n",
      "sayceratops\n",
      "giranosau\n",
      "epoch : 40, loss 1.2779937982559204\n",
      "macrocelta\n",
      "stinoceratops\n",
      "lioceratops\n",
      "tinodon\n",
      "cracosaurus\n",
      "siliosaurus\n",
      "lengonia\n",
      "saurus\n",
      "guneosaurus\n",
      "pro\n",
      "epoch : 41, loss 1.265514850616455\n",
      "macratops\n",
      "orwingosaurus\n",
      "tenannanahenitar\n",
      "ishazroston\n",
      "suulonia\n",
      "salpasaurus\n",
      "tianchusaurus\n",
      "sinosaurus\n",
      "t\n",
      "epoch : 42, loss 1.2551883459091187\n",
      "kusaurus\n",
      "nophodon\n",
      "huaxingosaurus\n",
      "palaeosaurus\n",
      "tengorophosaurus\n",
      "mononyx\n",
      "sauropeus\n",
      "dimugnosaurus\n",
      "peloc\n",
      "epoch : 43, loss 1.242828369140625\n",
      "niceratops\n",
      "artodon\n",
      "pinanzhops\n",
      "tinojapsaurus\n",
      "nibelesaurus\n",
      "ceratodon\n",
      "susimirus\n",
      "ticeratops\n",
      "tyronykus\n",
      "br\n",
      "epoch : 44, loss 1.2299984693527222\n",
      "s\n",
      "laphorsaurus\n",
      "shangonisaurus\n",
      "peloceratops\n",
      "momontholisaurus\n",
      "timannantanias\n",
      "tewynosyusops\n",
      "sterosaurus\n",
      "epoch : 45, loss 1.2205675840377808\n",
      "pelta\n",
      "plecropeltes\n",
      "tabadisaurus\n",
      "eurotyrannosaurus\n",
      "tyranosaurus\n",
      "prothosaurus\n",
      "sillosaurus\n",
      "kaleposaurus\n",
      "epoch : 46, loss 1.2082481384277344\n",
      "saurus\n",
      "protochion\n",
      "phgowenas\n",
      "riliasaurus\n",
      "sthesosaurus\n",
      "nophiosaurus\n",
      "channasaurus\n",
      "dasaurus\n",
      "muguacuati\n",
      "s\n",
      "epoch : 47, loss 1.1964014768600464\n",
      "asaurus\n",
      "timanosaurus\n",
      "archinoceratops\n",
      "terunosaurus\n",
      "sinoplong\n",
      "tianchinax\n",
      "satitan\n",
      "raychodon\n",
      "pinovenator\n",
      "epoch : 48, loss 1.185893177986145\n",
      "quizosaurus\n",
      "tyronosaurus\n",
      "arhorophus\n",
      "dianosaurus\n",
      "trinocona\n",
      "rudocasaurus\n",
      "sinornis\n",
      "ganthous\n",
      "trishops\n",
      "co\n",
      "epoch : 49, loss 1.1732763051986694\n",
      "n\n",
      "thomimosaurus\n",
      "sinomadros\n",
      "deeyangosaurus\n",
      "cryptosaurus\n",
      "kutanoceratops\n",
      "anchionimas\n",
      "sanatitan\n",
      "venator\n",
      "\n",
      "epoch : 50, loss 1.1645159721374512\n",
      "hangxausaurus\n",
      "brabiolin\n",
      "kugastegos\n",
      "eurosauravisaurus\n",
      "taeuistes\n",
      "senathoceratops\n",
      "borinosaurus\n",
      "shuecoto\n",
      "epoch : 51, loss 1.152100920677185\n",
      "canthus\n",
      "siamosaurus\n",
      "trachodon\n",
      "prorocosaurus\n",
      "thorophosaurus\n",
      "odonosaurus\n",
      "suapeota\n",
      "angasaurus\n",
      "herwathos\n",
      "epoch : 52, loss 1.139871597290039\n",
      "\n",
      "borenisaurus\n",
      "torosaurus\n",
      "salamosaurus\n",
      "heishanosaurusaurus\n",
      "microsaurus\n",
      "chianghia\n",
      "morontasaurus\n",
      "tarodo\n",
      "epoch : 53, loss 1.1297534704208374\n",
      "walaedicax\n",
      "allionadon\n",
      "tanarosaurus\n",
      "lipcosaurus\n",
      "huamisaurus\n",
      "migrapariosaurus\n",
      "shannanosaurus\n",
      "rurinacor\n",
      "epoch : 54, loss 1.1198655366897583\n",
      "\n",
      "norhoiue\n",
      "hengylosaurus\n",
      "pronyx\n",
      "steordor\n",
      "palacosaurus\n",
      "shandodon\n",
      "ornithomimus\n",
      "ginelosaurus\n",
      "caenosaurus\n",
      "epoch : 55, loss 1.106203556060791\n",
      "rus\n",
      "teratophyphasus\n",
      "strconotor\n",
      "venator\n",
      "xinabrosaurus\n",
      "caratosaurus\n",
      "prochenisaurus\n",
      "saipodon\n",
      "chuannaneu\n",
      "epoch : 56, loss 1.0965620279312134\n",
      "tussiceratops\n",
      "protanchisaurus\n",
      "brachisaurus\n",
      "sueleosaurus\n",
      "tarchisaurus\n",
      "torimus\n",
      "sataplatesia\n",
      "oplysaurop\n",
      "epoch : 57, loss 1.089748501777649\n",
      "brachia\n",
      "palteosaurus\n",
      "dramitosaurus\n",
      "tegangogia\n",
      "lugnusaurus\n",
      "stenosaurus\n",
      "therosaurus\n",
      "triaonosaurus\n",
      "shan\n",
      "epoch : 58, loss 1.073229193687439\n",
      "xisatros\n",
      "protarontosaurus\n",
      "thecovesaurus\n",
      "nekoetitan\n",
      "wanzhylostosudros\n",
      "tenotorosaurus\n",
      "tiangyloplongosa\n",
      "epoch : 59, loss 1.063266396522522\n",
      "osaurus\n",
      "tibaresaurus\n",
      "nanjiangohansus\n",
      "strgocephalosaurus\n",
      "ximosaurus\n",
      "arphosaurus\n",
      "sinoraptor\n",
      "paronocera\n",
      "epoch : 60, loss 1.056162714958191\n",
      "osaurus\n",
      "shunguiaus\n",
      "sinops\n",
      "torus\n",
      "oryptoscelosaurus\n",
      "tenantaor\n",
      "plutianshus\n",
      "oryptor\n",
      "irmudrodon\n",
      "protycera\n",
      "epoch : 61, loss 1.0434744358062744\n",
      "tenykus\n",
      "europetrood\n",
      "lus\n",
      "mohiodes\n",
      "sructodon\n",
      "dornianator\n",
      "fflanus\n",
      "mamengosaurus\n",
      "cryptosaurus\n",
      "protocerat\n",
      "epoch : 62, loss 1.0322619676589966\n",
      "venator\n",
      "venesaurus\n",
      "livheurus\n",
      "protomnia\n",
      "auropelta\n",
      "crachyrophelostyrurosaurus\n",
      "shansaurus\n",
      "sinomimus\n",
      "sci\n",
      "epoch : 63, loss 1.0237451791763306\n",
      "isaurus\n",
      "trachodon\n",
      "purusaurus\n",
      "tianchungosaurus\n",
      "siniomingosaurus\n",
      "pachyrritator\n",
      "isuchus\n",
      "ronisaurus\n",
      "tian\n",
      "epoch : 64, loss 1.014891505241394\n",
      "rus\n",
      "loritosaurus\n",
      "leocelassus\n",
      "iutasaurus\n",
      "nocodon\n",
      "monosaurus\n",
      "lishansaurus\n",
      "sangasaurus\n",
      "sthenosaurus\n",
      "ste\n",
      "epoch : 65, loss 1.0023159980773926\n",
      "long\n",
      "hanjingosaurus\n",
      "sinorontosaurus\n",
      "orotosaurus\n",
      "tionchosaurus\n",
      "protarrasaurus\n",
      "spinophoraptor\n",
      "panaroti\n",
      "epoch : 66, loss 0.9940679669380188\n",
      "nosaurus\n",
      "hypslongosaurus\n",
      "hyapes\n",
      "gangalosaurus\n",
      "theglops\n",
      "doplosaurus\n",
      "braxianosaurus\n",
      "traceratops\n",
      "megona\n",
      "epoch : 67, loss 0.9813232421875\n",
      "chisaurus\n",
      "protansaurus\n",
      "huaxiasaurus\n",
      "palaeosaurus\n",
      "jianganasaurus\n",
      "shancheiraptor\n",
      "gojingsaurus\n",
      "stenotos\n",
      "epoch : 68, loss 0.9732945561408997\n",
      "gousauru\n",
      "\n",
      "pirtholosuchus\n",
      "protochus\n",
      "vapioceratops\n",
      "kuntores\n",
      "ornithomimus\n",
      "susiliosaurus\n",
      "spinosaurus\n",
      "ste\n",
      "epoch : 69, loss 0.9617241024971008\n",
      "quintavisaurus\n",
      "qiagnathus\n",
      "protochaurus\n",
      "palaeontosaurus\n",
      "hepenosaurus\n",
      "syanosaurus\n",
      "igokosaurus\n",
      "cryptosa\n",
      "epoch : 70, loss 0.9554434418678284\n",
      "dros\n",
      "chynelopholass\n",
      "goplamusaurus\n",
      "stenotasaurus\n",
      "sinotoraptor\n",
      "gectanodon\n",
      "trinmbusaurus\n",
      "spinoclong\n",
      "san\n",
      "epoch : 71, loss 0.947073757648468\n",
      "yosaurus\n",
      "cranosaurus\n",
      "spanosaurus\n",
      "titonodon\n",
      "tranosaurus\n",
      "eucosaurus\n",
      "antrosaurus\n",
      "oponosaurus\n",
      "haklusauru\n",
      "epoch : 72, loss 0.9361414909362793\n",
      "venator\n",
      "grininitas\n",
      "pinthousaurus\n",
      "walwalochisaurus\n",
      "sianchang\n",
      "anochisaurus\n",
      "tiangunatitan\n",
      "fabakhingosau\n",
      "epoch : 73, loss 0.924109697341919\n",
      "guanodon\n",
      "projianosaurus\n",
      "trendallisaurus\n",
      "teratophonosaurus\n",
      "trichiosaurus\n",
      "tahadiaasaurus\n",
      "gryptosaurus\n",
      "\n",
      "epoch : 74, loss 0.9201676249504089\n",
      "wanghanosaurus\n",
      "thuchus\n",
      "taectops\n",
      "ladongosaurus\n",
      "sinorex\n",
      "salodontasaurus\n",
      "heawilosaurus\n",
      "pelarosaurus\n",
      "hua\n",
      "epoch : 75, loss 0.9058310389518738\n",
      "\n",
      "breitoasaurus\n",
      "prouantasaurus\n",
      "niolosaurus\n",
      "shagosaurus\n",
      "torelamosaurus\n",
      "drltacops\n",
      "rurilochaurus\n",
      "wultera\n",
      "epoch : 76, loss 0.9002042412757874\n",
      "anosaurus\n",
      "thaongosaurus\n",
      "nioraptor\n",
      "pecosaurus\n",
      "mustacelasaurus\n",
      "procantosaurus\n",
      "sminuondkonnatos\n",
      "olivela\n",
      "epoch : 77, loss 0.8869877457618713\n",
      "quing\n",
      "eraviceratops\n",
      "azklongosaurus\n",
      "sallosaurus\n",
      "tyangosaurus\n",
      "sinoceratops\n",
      "biangshanosaurus\n",
      "ntacosauru\n",
      "epoch : 78, loss 0.8854234218597412\n",
      "x\n",
      "erthenosaurus\n",
      "tigalueotidan\n",
      "lengosaurus\n",
      "tikarnis\n",
      "teaoningosaurus\n",
      "sauroplong\n",
      "tiansuchus\n",
      "ptiraxosaur\n",
      "epoch : 79, loss 0.8722139000892639\n",
      "wulanosaura\n",
      "othosutosuchus\n",
      "rachiomisosaurus\n",
      "cruchodon\n",
      "palagosaurus\n",
      "dystrobisaurus\n",
      "spinophosaurus\n",
      "ana\n",
      "epoch : 80, loss 0.8634288311004639\n",
      "nonghianylus\n",
      "tricilormisug\n",
      "aurornithomimus\n",
      "sinospondylus\n",
      "macroosaurus\n",
      "silorosaurus\n",
      "timicosaurus\n",
      "ther\n",
      "epoch : 81, loss 0.8570210337638855\n",
      "ceratosaurus\n",
      "shanosaurus\n",
      "thachips\n",
      "ardolysaurus\n",
      "suemos\n",
      "aldrosaurus\n",
      "strgosaurus\n",
      "turulosaurus\n",
      "titanosau\n",
      "epoch : 82, loss 0.8523039221763611\n",
      "mosaurus\n",
      "scandolosaurus\n",
      "magunosaurus\n",
      "lesionas\n",
      "sauromoches\n",
      "oshyceious\n",
      "megaonatus\n",
      "micronosaurus\n",
      "proche\n",
      "epoch : 83, loss 0.8396697044372559\n",
      "haeus\n",
      "triatosaurus\n",
      "protoanisaurus\n",
      "singosaurus\n",
      "sinornitholepiator\n",
      "isuchus\n",
      "poloheidosaurus\n",
      "pelecinicho\n",
      "epoch : 84, loss 0.8265976905822754\n",
      "n\n",
      "boevisaurus\n",
      "tenatosaurus\n",
      "tiangosaurus\n",
      "tomasuaia\n",
      "kelarocerhaor\n",
      "arkhosaurus\n",
      "sinisanasaurus\n",
      "chuintosa\n",
      "epoch : 85, loss 0.8273410201072693\n",
      "handylostrosaurus\n",
      "peleosaurus\n",
      "timblachosoplosaurosaurus\n",
      "sinosauros\n",
      "pherosaurosaurus\n",
      "timus\n",
      "taichodon\n",
      "\n",
      "epoch : 86, loss 0.818041980266571\n",
      "trianosaurus\n",
      "tengonavenator\n",
      "suerisaurus\n",
      "coelurisauramosaurus\n",
      "shanosaurus\n",
      "tangthon\n",
      "dostichasiomestasa\n",
      "epoch : 87, loss 0.8056101202964783\n",
      "kosaurus\n",
      "tanotyrannus\n",
      "niankaun\n",
      "kigantania\n",
      "golisaurus\n",
      "ratanosaurus\n",
      "thamngrosaurus\n",
      "titakosaurus\n",
      "tharon\n",
      "epoch : 88, loss 0.794727087020874\n",
      "zasaurus\n",
      "nourbyasaurus\n",
      "liaoningosaurus\n",
      "wulengosaurus\n",
      "sheposaurus\n",
      "gianovenator\n",
      "ombgodadon\n",
      "tongilasaur\n",
      "epoch : 89, loss 0.7973770499229431\n",
      "don\n",
      "steusaurus\n",
      "sitrosaurus\n",
      "siggnator\n",
      "mongongorantis\n",
      "euiparosaurus\n",
      "timannatatr\n",
      "scrusethalaeratosaurus\n",
      "epoch : 90, loss 0.7847364544868469\n",
      "ellodon\n",
      "eosacrosaurus\n",
      "surconykus\n",
      "borainosaurus\n",
      "hubliciodon\n",
      "dorachitacasaurus\n",
      "thisaurus\n",
      "morhinocerato\n",
      "epoch : 91, loss 0.7797484397888184\n",
      "urus\n",
      "haplosaurus\n",
      "shanosaurus\n",
      "tiangsuanous\n",
      "miceratops\n",
      "brohisaurus\n",
      "monodon\n",
      "brachiamisaurus\n",
      "shinocerato\n",
      "epoch : 92, loss 0.7672045230865479\n",
      "quitosaurus\n",
      "stenonosaurus\n",
      "theposaurus\n",
      "tendodoniosaurus\n",
      "spanosaurus\n",
      "tohalmis\n",
      "salberatans\n",
      "prytosaurus\n",
      "\n",
      "epoch : 93, loss 0.7624070048332214\n",
      "urus\n",
      "tianchisaurus\n",
      "ongosaurus\n",
      "gorioriosaurus\n",
      "procorosaurus\n",
      "ornithoides\n",
      "ornithopelta\n",
      "brachiosaurus\n",
      "sc\n",
      "epoch : 94, loss 0.7593564987182617\n",
      "jaanis\n",
      "atoposaurus\n",
      "anganatitan\n",
      "trialestes\n",
      "serindexia\n",
      "phalleptatyrinstonaria\n",
      "shinosaurus\n",
      "therosaurur\n",
      "\n",
      "epoch : 95, loss 0.7465680241584778\n",
      "yangosaurus\n",
      "temodon\n",
      "linasaurus\n",
      "lapacomasaurus\n",
      "ranoceratops\n",
      "anzhynosaurus\n",
      "menodecaptor\n",
      "xinnehuasaurus\n",
      "epoch : 96, loss 0.7447138428688049\n",
      "mimus\n",
      "orhatsaurus\n",
      "lepidus\n",
      "broceratops\n",
      "iniasauropes\n",
      "elosauroposaurus\n",
      "siameosaurus\n",
      "tianksteton\n",
      "geumacr\n",
      "epoch : 97, loss 0.7327747344970703\n",
      "pantesaurosaurus\n",
      "siomosaurus\n",
      "scherodontosaurus\n",
      "siuvalachiorestes\n",
      "dryodeosaurus\n",
      "trinicoraptor\n",
      "glacera\n",
      "epoch : 98, loss 0.7289780974388123\n",
      "wasshassaurus\n",
      "uraclotyrausus\n",
      "llosimaestylosous\n",
      "mechiosaurus\n",
      "temanosaurus\n",
      "tonotasaurus\n",
      "niopsaurus\n",
      "tia\n",
      "epoch : 99, loss 0.7211578488349915\n",
      "ylostops\n",
      "aptorylophus\n",
      "fengusaurus\n",
      "titania\n",
      "ohadrosaurus\n",
      "therinosaurus\n",
      "thrcolosaurus\n",
      "terpotadan\n",
      "rukind\n",
      "epoch : 100, loss 0.71864253282547\n",
      "lonia\n",
      "salasougryptor\n",
      "onganosaurus\n",
      "tergonimosaurus\n",
      "hampliosaurus\n",
      "tianchusaurus\n",
      "microsaurus\n",
      "lapathocep\n"
     ]
    }
   ],
   "source": [
    "if __name__ ==  \"__main__\":\n",
    "\n",
    "    data = open('datasets/dinos.txt', 'r').read()\n",
    "    data = data.lower()\n",
    "\n",
    "    seq_length  = 25\n",
    "    batch_size = 64\n",
    "    hidden_size = 256\n",
    "\n",
    "    text_dataset = TextDataset(data, seq_length=seq_length)\n",
    "    text_dataloader = DataLoader(text_dataset, batch_size)\n",
    "\n",
    "    rnnModel = RNN(len(text_dataset.chars), hidden_size, len(text_dataset.chars),batch_size)\n",
    "\n",
    "    epochs = 100\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(rnnModel.parameters(), lr=0.001)\n",
    "\n",
    "    train(rnnModel, text_dataloader, epochs, optimizer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d54116-0903-4b70-a5b2-7df3db7aa256",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
